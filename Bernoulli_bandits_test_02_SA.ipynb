{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 BERNOULLI BANDITS FIXED POLICY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIN STAY LOOSE SHIFT (WSLS) benchmark problem.\n",
    "\n",
    "Controller size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dimS=2\n",
    "dimA=2\n",
    "dimQ=2\n",
    "dimO=2\n",
    "p=0.8\n",
    "\n",
    "\n",
    "states=np.arange(0,dimS,dtype=int)\n",
    "actions=np.arange(0,dimA,dtype=int)\n",
    "nodes=np.arange(0,dimQ,dtype=int)\n",
    "obs=np.arange(0,dimO,dtype=int)\n",
    "\n",
    "\n",
    "# p(s'|sa)\n",
    "env_prob=np.zeros((dimS,dimA,dimS))\n",
    "\n",
    "for stateprime in states:\n",
    "    for action in actions:\n",
    "        for state in states:\n",
    "            if (stateprime == state): env_prob[stateprime,state,action]=1\n",
    "\n",
    "\n",
    "# p(a|q) Here's the policty of the WSLS strategy\n",
    "policy=np.zeros((dimA,dimQ))\n",
    "\n",
    "for action in actions:\n",
    "    for node in nodes:\n",
    "        if (action == node): policy[action,node]=1\n",
    "\n",
    "#f(o|s'a)\n",
    "obs_prob=np.zeros((dimO,dimS,dimA))\n",
    "\n",
    "for stateprime in states:\n",
    "    for o in obs:\n",
    "        for action in actions:\n",
    "            if (stateprime==action): \n",
    "                obs_prob[o,stateprime,action]=p**o*(1.0-p)**(1-o)\n",
    "            else:\n",
    "                obs_prob[o,stateprime,action]=(1.0-p)**o*p**(1.0-o)\n",
    "\n",
    "#g(q'|qao) Here's the memory update of the WSLS strategy\n",
    "update=np.zeros((dimQ,dimA,dimQ,dimO))\n",
    "\n",
    "for nodeprime in nodes:\n",
    "    for node in nodes:\n",
    "        for action in actions:\n",
    "            for o in obs:\n",
    "                if (o==1 and node==nodeprime): update[nodeprime,action,node,o]=1\n",
    "                if (o==0 and node!=nodeprime): update[nodeprime,action,node,o]=1\n",
    "                \n",
    "#r(sa) Average reward\n",
    "rew=np.zeros((dimS,dimA))\n",
    "\n",
    "rew[0,0]=p\n",
    "rew[0,1]=1-p\n",
    "rew[1,0]=1-p\n",
    "rew[1,1]=p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a benchmark problem the policy and the memory update have been fixed. The Bellman quaratic constraint boils down to be a linear system. The solution of this system is the value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [[1.48 0.88]\n",
      " [0.88 1.48]]\n",
      "=============\n",
      "Objective function: 1.18\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "#b(sq)\n",
    "init_node=0\n",
    "gamma=0.5\n",
    "b=np.matmul(rew,policy)\n",
    "\n",
    "#Bellman equation linear constraint\n",
    "T=np.zeros((dimQ,dimS,dimQ,dimS))\n",
    "Id=np.zeros((dimQ,dimS,dimQ,dimS))\n",
    "\n",
    "#T(sq|s'q')=sum_a sum_y (p(s'|sa)*f(o|sa)*g(q'|qao)*policy(a|q))\n",
    "for node in nodes:\n",
    "    for state in states:\n",
    "        for nodeprime in nodes:\n",
    "            for stateprime in states:\n",
    "                for o in obs:\n",
    "                    for action in actions:\n",
    "                        T[node,state,nodeprime,stateprime]+=update[nodeprime,action,node,o]*policy[action,node]*env_prob[stateprime,state,action]*obs_prob[o,stateprime,action]\n",
    "\n",
    "#Id=delta(ss',qq')\n",
    "for nodeprime in nodes:\n",
    "    for stateprime in states:\n",
    "        for node in nodes:\n",
    "            for state in states:\n",
    "                if (state==stateprime and node==nodeprime): Id[node,state,nodeprime,stateprime]=1\n",
    "\n",
    "#delta(ss',qq')- gamma*[sum_a sum_y (p(s'|sa)*f(o|sa)*g(q'|qao)*policy(a|q))]\n",
    "Btens=Id-gamma*T\n",
    "\n",
    "#Solve the linear system to get the single value solution\n",
    "v=np.linalg.tensorsolve(Btens,b)\n",
    "print(\"Value:\",v)\n",
    "print(\"=============\")\n",
    "\n",
    "#Calculation of the objective function with fixed inotial node\n",
    "print(\"Objective function:\", 0.5*np.sum(v[init_node,:]))\n",
    "\n",
    "#Def normalized x(q',a,q,o)\n",
    "x=np.zeros((dimQ,dimA,dimQ,dimO))\n",
    "for nodeprime in nodes:\n",
    "    for action in actions:\n",
    "        for node in nodes:\n",
    "            for o in obs:\n",
    "                x[nodeprime,action,node,o]=update[nodeprime,action,node,o]*policy[action,node]\n",
    "\n",
    "print(x[1,1,1,0],x[1,1,1,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section the same calculations are carried out. In this case rank two arrays have been used to evalute the Bellman equation. \n",
    "\n",
    "\n",
    "The \"Bellman matrix\" (Bmat) is block diagonal, which makes sense because the transition probability from one stae to the other is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial mean value   >>>>>>>>>>\n",
      "1.18\n",
      "Initial policy       >>>>>>>>>>\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "value                >>>>>>>>>>>\n",
      "[[1.625197   0.99755214]\n",
      " [0.93697533 1.42047704]]\n",
      "mean value           >>>>>>>>>>>\n",
      "1.3113745684523215\n",
      "objective function   >>>>>>>>>>>\n",
      "-1.1824918870964816\n"
     ]
    }
   ],
   "source": [
    "from f2py_jit import jit\n",
    "import numpy as np\n",
    "f90=jit(\"SA_QCLP_ut_02_cor1.f90\",flags=\"-O2 -fcheck=bounds\")\n",
    "\n",
    "#initial value for the x(q'aqo)\n",
    "x_init=x\n",
    "#initial value for the y(qs)\n",
    "y_init=v\n",
    "#Initial value of objective function\n",
    "val_init=0.5*np.sum(y_init[init_node,:])\n",
    "\n",
    "print(\"Initial mean value   >>>>>>>>>>\")\n",
    "print(val_init)\n",
    "print(\"Initial policy       >>>>>>>>>>\")\n",
    "print(policy)\n",
    "\n",
    "#Initial parameters\n",
    "T0=50\n",
    "T_end=1e-6\n",
    "Temp_factor=0.6\n",
    "nstep=1000\n",
    "pen_step=200\n",
    "N=1\n",
    "stdev=1.0\n",
    "\n",
    "for p in range(N):\n",
    "\n",
    "    sigma=2.0\n",
    "    mu=2.0\n",
    "\n",
    "    #x(q'aqo) sample size and generation\n",
    "    size_gauss_x=np.round(dimO*nstep*(1+np.log(T_end/T0)/np.log(Temp_factor))).astype(int)\n",
    "   \n",
    "    #Generarion of gauss sample. In this way the point generated is independent from o\n",
    "    x_gauss=np.random.normal(0.0,stdev,size=(dimQ,dimA,dimQ,pen_step*size_gauss_x))        \n",
    "    #Generating the effective gauss_sample                 \n",
    "\n",
    "    #y(qs) sample size and generation\n",
    "    size_gauss_y=np.round(dimS*nstep*(1+np.log(T_end/T0)/np.log(Temp_factor))).astype(int)\n",
    "    y_gauss=np.random.normal(0.0,stdev,size=(dimQ,pen_step*size_gauss_y))\n",
    "\n",
    "    #Simulated Annealing \n",
    "    Res=f90.qclp.drive(T0,T_end,Temp_factor,nstep,pen_step,x_init,y_init,val_init,rew,env_prob,obs_prob,sigma,mu,x_gauss,y_gauss,gamma)\n",
    "\n",
    "x_min=Res[0]\n",
    "y_min=Res[1]\n",
    "print(\"value                >>>>>>>>>>>\")\n",
    "print(y_min)\n",
    "print(\"mean value           >>>>>>>>>>>\")\n",
    "print(0.5*np.sum(y_min[init_node,:]))\n",
    "print(\"objective function   >>>>>>>>>>>\")\n",
    "print(Res[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.625197   0.99755214]\n",
      " [0.93697533 1.42047704]]\n",
      "[[0.90703248 0.07559685]\n",
      " [0.09296752 0.92440315]]\n",
      "[[0.91422573 0.07269337]\n",
      " [0.08577427 0.92730663]]\n",
      "norm [[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_min)\n",
    "Pol0=np.zeros((dimA,dimQ))\n",
    "Pol1=np.zeros((dimA,dimQ))\n",
    "\n",
    "for action in actions:\n",
    "    for node in nodes:\n",
    "        for nodeprime in nodes:\n",
    "            Pol0[action,node]=np.sum(x_min[:,action,node,0])\n",
    "\n",
    "for action in actions:\n",
    "    for node in nodes:\n",
    "        Pol1[action,node]=np.sum(x_min[:,action,node,1])\n",
    "\n",
    "\n",
    "print(Pol0)\n",
    "print(Pol1)\n",
    "\n",
    "normalization=np.zeros((dimQ,dimO))\n",
    "\n",
    "for node in nodes:\n",
    "    for o in obs:\n",
    "        for nodeprime in nodes:\n",
    "            for action in actions:\n",
    "                normalization[node,o]=np.sum(x_min[:,:,node,o])\n",
    "\n",
    "print(\"norm\",normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dimS=2\n",
    "dimA=2\n",
    "dimQ=3\n",
    "dimO=2\n",
    "p=0.8\n",
    "\n",
    "\n",
    "states=np.arange(0,dimS,dtype=int)\n",
    "actions=np.arange(0,dimA,dtype=int)\n",
    "nodes=np.arange(0,dimQ,dtype=int)\n",
    "obs=np.arange(0,dimO,dtype=int)\n",
    "\n",
    "\n",
    "# p(s'|sa)\n",
    "env_prob=np.zeros((dimS,dimA,dimS))\n",
    "\n",
    "for stateprime in states:\n",
    "    for action in actions:\n",
    "        for state in states:\n",
    "            if (stateprime == state): env_prob[stateprime,state,action]=1\n",
    "\n",
    "\n",
    "# p(a|q) Here's the policty of the WSLS strategy\n",
    "policy=np.zeros((dimA,dimQ))\n",
    "\n",
    "for action in actions:\n",
    "    for node in nodes:\n",
    "        if (action == node):\n",
    "            policy[action,node]=1\n",
    "        elif (node==2):\n",
    "            policy[action,node]=0.5\n",
    "        else:\n",
    "            policy[action,node]=0\n",
    "\n",
    "#f(o|s'a)\n",
    "obs_prob=np.zeros((dimO,dimS,dimA))\n",
    "\n",
    "for stateprime in states:\n",
    "    for o in obs:\n",
    "        for action in actions:\n",
    "            if (stateprime==action): \n",
    "                obs_prob[o,stateprime,action]=p**o*(1.0-p)**(1-o)\n",
    "            else:\n",
    "                obs_prob[o,stateprime,action]=(1.0-p)**o*p**(1.0-o)\n",
    "\n",
    "#g(q'|qao) Here's the memory update of the WSLS strategy\n",
    "update=np.zeros((dimQ,dimA,dimQ,dimO))\n",
    "\n",
    "for nodeprime in nodes:\n",
    "    for action in actions:\n",
    "        for node in nodes:\n",
    "            for o in obs:\n",
    "                # if we win in the node corresponding to action, we stay\n",
    "                if nodeprime == node and node == action and o == 1:\n",
    "                    update[nodeprime,action,node,o] = 1\n",
    "                # if we lose in the node corresponding to action, we shift to the intermediate node\n",
    "                elif nodeprime == 2 and node == action and o == 0:\n",
    "                    update[nodeprime,action,node,o] = 1\n",
    "                # if we win in the intermediate node, we shift to node corresponding to action taken\n",
    "                elif nodeprime == action and node == 2 and o == 1:\n",
    "                    update[nodeprime,action,node,o] = 1 # P(a | 2) = 0.5\n",
    "                # if we lose in the intermediate node, we shift to node corresponding to the other action\n",
    "                elif nodeprime!= action and node == 2 and nodeprime != 2 and o == 0:\n",
    "                    update[nodeprime,action,node,o] = 1 # P(a | 2) = 0.5\n",
    "                # the rest is 0\n",
    "                else:\n",
    "                    update[nodeprime,action,node,o] = 0\n",
    "                \n",
    "#r(sa) Average reward\n",
    "rew=np.zeros((dimS,dimA))\n",
    "\n",
    "rew[0,0]=p\n",
    "rew[0,1]=1-p\n",
    "rew[1,0]=1-p\n",
    "rew[1,1]=p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [[1.53125 0.75   ]\n",
      " [0.75    1.53125]\n",
      " [1.1875  1.1875 ]]\n",
      "=============\n",
      "Objective function: 1.140625\n"
     ]
    }
   ],
   "source": [
    "#b(sq)\n",
    "init_node=0\n",
    "gamma=0.5\n",
    "b=np.zeros((dimQ,dimS))\n",
    "\n",
    "for node in nodes:\n",
    "    for state in states:\n",
    "        for action in actions:\n",
    "            b[node,state]+=rew[state,action]*policy[action,node]\n",
    "\n",
    "\n",
    "#Bellman equation linear constraint\n",
    "T=np.zeros((dimQ,dimS,dimQ,dimS))\n",
    "Id=np.zeros((dimQ,dimS,dimQ,dimS))\n",
    "\n",
    "#T(qs|q's')=sum_a sum_y (p(s'|sa)*f(o|sa)*g(q'|qao)*policy(a|q))\n",
    "for node in nodes:\n",
    "    for state in states:\n",
    "        for nodeprime in nodes:\n",
    "            for stateprime in states:\n",
    "                for o in obs:\n",
    "                    for action in actions:\n",
    "                        T[node,state,nodeprime,stateprime]+=update[nodeprime,action,node,o]*policy[action,node]*env_prob[stateprime,state,action]*obs_prob[o,stateprime,action]\n",
    "\n",
    "#Id=delta(ss',qq')\n",
    "for nodeprime in nodes:\n",
    "    for stateprime in states:\n",
    "        for node in nodes:\n",
    "            for state in states:\n",
    "                if (state==stateprime and node==nodeprime): Id[node,state,nodeprime,stateprime]=1\n",
    "\n",
    "#delta(ss',qq')- gamma*[sum_a sum_y (p(s'|sa)*f(o|sa)*g(q'|qao)*policy(a|q))]\n",
    "Btens=Id-gamma*T\n",
    "\n",
    "#Solve the linear system to get the single value solution\n",
    "v=np.linalg.tensorsolve(Btens,b)\n",
    "print(\"Value:\",v)\n",
    "print(\"=============\")\n",
    "\n",
    "#Calculation of the objective function with fixed inotial node\n",
    "print(\"Objective function:\", 0.5*np.sum(v[init_node,:]))\n",
    "\n",
    "#Def normalized x(q',a,q,o)\n",
    "x=np.zeros((dimQ,dimA,dimQ,dimO))\n",
    "for nodeprime in nodes:\n",
    "    for action in actions:\n",
    "        for node in nodes:\n",
    "            for o in obs:\n",
    "                x[nodeprime,action,node,o]=update[nodeprime,action,node,o]*policy[action,node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial mean value   >>>>>>>>>>\n",
      "1.140625\n",
      "Initial policy       >>>>>>>>>>\n",
      "[[1.  0.  0.5]\n",
      " [0.  1.  0.5]]\n",
      "value                >>>>>>>>>>>\n",
      "[[1.45055557 0.90821086]\n",
      " [0.6444188  1.55060089]\n",
      " [0.71110304 1.53534958]]\n",
      "mean value           >>>>>>>>>>>\n",
      "1.1793832137219378\n",
      "objective function   >>>>>>>>>>>\n",
      "-1.1782832562209606\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from f2py_jit import jit\n",
    "f90=jit(\"SA_QCLP_ut_02_cor1.f90\",flags=\"-O3 -fcheck=bounds\")\n",
    "\n",
    "#initial value for the x(q'aqo)\n",
    "x_init=x\n",
    "#initial value for the y(qs)\n",
    "y_init=v\n",
    "#Initial value of objective function\n",
    "val_init=0.5*np.sum(y_init[init_node,:])\n",
    "\n",
    "print(\"Initial mean value   >>>>>>>>>>\")\n",
    "print(val_init)\n",
    "print(\"Initial policy       >>>>>>>>>>\")\n",
    "print(policy)\n",
    "\n",
    "#Initial parameters\n",
    "T0=10\n",
    "T_end=1e-6\n",
    "Temp_factor=0.6\n",
    "factor=0.3\n",
    "nstep=500\n",
    "pen_step=1000\n",
    "N=1\n",
    "stdev=1.0\n",
    "\n",
    "for p in range(N):\n",
    "\n",
    "    sigma=2.0\n",
    "    mu=2.0\n",
    "\n",
    "    #x(q'aqo) sample size and generation\n",
    "    size_gauss_x=np.round(dimO*nstep*(1+np.log(T_end/T0)/np.log(Temp_factor))).astype(int)\n",
    "\n",
    "    #Generarion of gauss sample. In this way the point generated is independent from o\n",
    "    x_gauss=np.random.normal(0.0,stdev,size=(dimQ,dimA,dimQ,pen_step*size_gauss_x))        \n",
    "    #Generating the effective gauss_sample                 \n",
    "\n",
    "    #y(qs) sample size and generation\n",
    "    size_gauss_y=np.round(dimS*nstep*(1+np.log(T_end/T0)/np.log(Temp_factor))).astype(int)\n",
    "    y_gauss=np.random.normal(0.0,stdev,size=(dimQ,pen_step*size_gauss_y))\n",
    "\n",
    "    #Simulated Annealing \n",
    "    Res=f90.qclp.drive(T0,T_end,Temp_factor,nstep,pen_step,x_init,y_init,val_init,rew,env_prob,obs_prob,sigma,mu,x_gauss,y_gauss,gamma)\n",
    "\n",
    "\n",
    "    \n",
    "x_min=Res[0]\n",
    "y_min=Res[1]\n",
    "print(\"value                >>>>>>>>>>>\")\n",
    "print(y_min)\n",
    "print(\"mean value           >>>>>>>>>>>\")\n",
    "print(0.5*np.sum(y_min[init_node,:]))\n",
    "print(\"objective function   >>>>>>>>>>>\")\n",
    "print(Res[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.46144802 0.90041418]\n",
      " [0.70939496 1.53451892]\n",
      " [0.80898385 1.51079012]]\n",
      "[[0.99871362 0.00452143 0.00213783]\n",
      " [0.00128638 0.99547857 0.99786217]]\n",
      "[[0.99874143 0.00446269 0.00213568]\n",
      " [0.00125857 0.99553731 0.99786432]]\n",
      "norm [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_min)\n",
    "Pol0=np.zeros((dimA,dimQ))\n",
    "Pol1=np.zeros((dimA,dimQ))\n",
    "\n",
    "for action in actions:\n",
    "    for node in nodes:\n",
    "        for nodeprime in nodes:\n",
    "            Pol0[action,node]=np.sum(x_min[:,action,node,0])\n",
    "\n",
    "for action in actions:\n",
    "    for node in nodes:\n",
    "        Pol1[action,node]=np.sum(x_min[:,action,node,1])\n",
    "\n",
    "\n",
    "print(Pol0)\n",
    "print(Pol1)\n",
    "\n",
    "normalization=np.zeros((dimQ,dimO))\n",
    "\n",
    "for node in nodes:\n",
    "    for o in obs:\n",
    "        for nodeprime in nodes:\n",
    "            for action in actions:\n",
    "                normalization[node,o]=np.sum(x_min[:,:,node,o])\n",
    "\n",
    "print(\"norm\",normalization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
